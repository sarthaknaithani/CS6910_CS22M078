{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMEeBJLTeyKB"
      },
      "outputs": [],
      "source": [
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfQKQYGw697b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKAtDnkJxwF"
      },
      "source": [
        "### DOWNLOADING THE DATASET AND PLOTTING ONE SAMPLE IMAGE FROM EACH CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFpHsIZX697r"
      },
      "outputs": [],
      "source": [
        "def plot_images():\n",
        "  # start a new wandb run to track this script\n",
        "  wandb.init(project=\"CS6910_Assignment-1\")\n",
        "  # simulate training\n",
        "  # x_train is a (60000,28,28) matrix consisting image pixels for training\n",
        "  # y_train is a (60000,1) matrix consisting of labels\n",
        "  # x_test is a (10000,28,28) matrix consisting image pixels for testing\n",
        "  # y_test is a (10000,1) matrix consisting of labels for testing\n",
        "  (x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()\n",
        "  labels=set()\n",
        "  i=0\n",
        "  fig,ax=plt.subplots(2,5,figsize=(10,5))\n",
        "  row=0\n",
        "  col=0\n",
        "  for pixels in x_train:\n",
        "    #The matplotlib function imshow() creates an image from a 2-dimensional numpy array\n",
        "    #pixels is (28,28) 2-D array\n",
        "    #l is the current label of image\n",
        "    l=y_train[i]\n",
        "    if(not(l in labels)):\n",
        "      if(col>=5):\n",
        "        col=0\n",
        "        row+=1\n",
        "      ax[row][col].imshow(pixels,cmap=\"gray\")\n",
        "      ax[row][col].set_title(\"Label {}\".format(l))\n",
        "      ax[row][col].axis(False)\n",
        "      labels.add(l)\n",
        "      col+=1\n",
        "    #if we get all our 10 labels just break the loop\n",
        "    if(len(labels)==10):\n",
        "      break;\n",
        "    i+=1\n",
        "  wandb.log({\"plot\":plt})\n",
        "  wandb.run.name = \"Sample_Images\"\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "  # finish the wandb run, necessary in notebooks\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99oM1GulHrTg"
      },
      "source": [
        "## DATA-PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkEiIJbr697v"
      },
      "outputs": [],
      "source": [
        "def data_preprocess():\n",
        "    (x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()\n",
        "    #NORMALIZING THE DATASET\n",
        "    x_train=x_train/255.0\n",
        "    x_test=x_test/255.0\n",
        "    #RESHAPING THE TRAIN_IMAGE DATASET FROM (60000,28x28) TO (60000,784) AND SAME FOR TEST_IMAGE\n",
        "    num_inputs=784\n",
        "    num_outputs=10\n",
        "    x_train=x_train.reshape(60000,784)\n",
        "    x_test=x_test.reshape(10000,784)\n",
        "\n",
        "\n",
        "    #SPLITTING THE TRAINING DATA FOR VALIDATION AND TESTING\n",
        "    train_x,val_x,train_y,val_y=train_test_split(x_train,y_train)\n",
        "    train_x=np.transpose(train_x)\n",
        "    train_y=np.transpose(train_y)\n",
        "    val_x=np.transpose(val_x)\n",
        "    val_y=np.transpose(val_y)\n",
        "    #RESHAPING MY DATA TO COLUMN-WISE IMAGES\n",
        "    x_train=x_train.T\n",
        "    x_test=x_test.T\n",
        "    return x_train,y_train,train_x,train_y,val_x,val_y\n",
        "\n",
        "#ONE-HOT ENCODING FOR Y_TRAIN AND Y_TEST: \n",
        "def one_hot_encoding(y):\n",
        "    exp_y=np.zeros((10,y.shape[0]))\n",
        "    for i in range(0,y.shape[0]):\n",
        "        exp_y[y[i]][i]=1\n",
        "    return exp_y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsTgloNKlu1"
      },
      "source": [
        "### ACTIVATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rjPkhE9697x"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return np.multiply(s, np.subtract(1, s))\n",
        "\n",
        "def softmax(x):\n",
        "    x=x-np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def Relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def Relu_derivative(x):\n",
        "    return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwJriuPKz-F"
      },
      "source": [
        "### INITIALIZING THE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWQEwczB6972"
      },
      "outputs": [],
      "source": [
        "def initialize_params(hidden_layers,neurons,method):\n",
        "  #USING XAVIER INITIALIZATION TO INITIALIZE WEIGHTS AND BIAS MATRIX\n",
        "\n",
        "  #INDEXING DONE FROM 1\n",
        "  L=hidden_layers+1 #number of layers excluding hidden layer\n",
        "  weights=[0]*(hidden_layers+2)\n",
        "  biases=[0]*(hidden_layers+2)\n",
        "  previous_updates_W=[0]*(hidden_layers+2)\n",
        "  previous_updates_B=[0]*(hidden_layers+2)\n",
        "  np.random.seed(42)\n",
        "  for i in range(1,hidden_layers+1):\n",
        "    n=neurons[i]\n",
        "    # appending the weight and bias matrix for the ith layer\n",
        "    if(i==1):\n",
        "      if method=='xavier':\n",
        "        weights[i]=(np.random.randn(n,784)*np.sqrt(2/(n+784)))\n",
        "      if method=='random':\n",
        "        weights[i]=(np.random.randn(n,784))*0.01\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,784))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "    else:\n",
        "      if method=='xavier':\n",
        "        weights[i]=(np.random.randn(n,neurons[i-1])*np.sqrt(2/(n+neurons[i-1])))\n",
        "      if method=='random':\n",
        "        weights[i]=(np.random.randn(n,neurons[i-1]))*0.01\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,neurons[i-1]))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "  weights[L]=(np.random.randn(10,neurons[hidden_layers])*np.sqrt(2/(10+neurons[hidden_layers-1])))\n",
        "  biases[L]=(np.zeros((10,1)))\n",
        "  previous_updates_W[L]=np.zeros((10,neurons[hidden_layers]))\n",
        "  previous_updates_B[L]=np.zeros((10,1))\n",
        "  weights=np.array(weights,dtype=object)\n",
        "  biases=np.array(biases,dtype=object)\n",
        "  previous_updates_W=np.array(previous_updates_W,dtype=object)\n",
        "  previous_updates_B=np.array(previous_updates_B,dtype=object)\n",
        "  return weights,biases,previous_updates_W,previous_updates_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAe2kdSTLThK"
      },
      "source": [
        "## FEED FORWARD PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYfoVYJm6973"
      },
      "outputs": [],
      "source": [
        "def FeedForwardNetwork(weights,biases,L,data,activation):\n",
        "  #Returns the array containing the output probablity for each class the data can belong\n",
        "  a=[0]*(L+1)\n",
        "  h=[0]*(L+1)\n",
        "  h[0]=data\n",
        "  for i in range(1,L):\n",
        "    #weight and bias matrix for hidden-layer i\n",
        "    weight=weights[i]\n",
        "    bias=biases[i]\n",
        "    #pre activation\n",
        "    a[i]=(bias+np.matmul(weight,h[i-1]))\n",
        "    #post activation\n",
        "    if(activation=='sigmoid'):\n",
        "      h[i]=(sigmoid(a[i]))\n",
        "    if(activation=='relu'):\n",
        "      h[i]=(Relu(a[i]))\n",
        "    if(activation=='tanh'):\n",
        "      h[i]=(tanh(a[i]))\n",
        "  #weight and bias matrix for ouput layer\n",
        "  weight=weights[L]\n",
        "  bias=biases[L]\n",
        "  a[L]=(bias+np.matmul(weight,h[L-1]))\n",
        "  h[L]=softmax(a[L])\n",
        "  return h[L],h,a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MhyJnsLm2w"
      },
      "source": [
        "## BACK PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KvkdzaQ6975"
      },
      "outputs": [],
      "source": [
        "def BackPropogation(weights,L,H,A,exp_Y,y_hat,activation):\n",
        "  # Input Parameters of function\n",
        "  # --> Model Parameters: weights and biases\n",
        "  # --> H: Post-activations, A: Pre-Activations\n",
        "  # --> exp_Y: Actual Class Labels\n",
        "  # Returns list of gradients for each layer wrt weights and biases(parameters)\n",
        "  gradients_H=[0] * (L+1)\n",
        "  gradients_W=[0] * (L+1)\n",
        "  gradients_B=[0] * (L+1)\n",
        "  gradients_A=[0] * (L+1)\n",
        "  \n",
        "  #Computing Gradient For The Output Layer(Pre Activation)\n",
        "  gradients_A[L]=-(exp_Y-y_hat)\n",
        "  for k in range(L,0,-1):\n",
        "    #compute gradients of the parameters\n",
        "    gradients_W[k]=np.matmul(gradients_A[k],np.transpose(H[k-1]))\n",
        "    gradients_B[k]=np.sum(gradients_A[k], axis=1,keepdims=True)\n",
        "    #compute gradients of the hidden layers\n",
        "    gradients_H[k-1]=np.matmul(np.transpose(weights[k]),gradients_A[k])\n",
        "    if(k>1):\n",
        "      if activation=='sigmoid':\n",
        "        gradients_A[k-1]=np.multiply(gradients_H[k-1],sigmoid_derivative(A[k-1]))\n",
        "      if activation=='relu':\n",
        "        gradients_A[k-1]=np.multiply(gradients_H[k-1],Relu_derivative(A[k-1]))\n",
        "      if activation=='tanh':\n",
        "        gradients_A[k-1]=np.multiply(gradients_H[k-1],tanh_derivative(A[k-1]))\n",
        "    \n",
        "\n",
        "  return gradients_W,gradients_B"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LOSS AND ACCURACY FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(weights,y,exp_y,loss,data_size,L2_lamb):\n",
        "    # Calculating -log(p(x)) where p(x) is probablity where our of our actual class label of x\n",
        "    # exp_y contains y=1 if y is actual class of x else 0\n",
        "    # so dot product gives only the log of the probablity of actual class of x\n",
        "    loss_val=0\n",
        "    if loss=='cross_entropy':\n",
        "        p_x= np.multiply(exp_y,np.log(y))\n",
        "        loss_val= -np.sum(p_x)/data_size\n",
        "    if loss=='MSE':\n",
        "        p_x=(y-exp_y)**2\n",
        "        loss_val= 0.5 * np.sum(p_x)/data_size\n",
        "    # Applying L2_regulaization\n",
        "    square_sum=0\n",
        "    for i in range(1,len(weights)):\n",
        "        square_sum+=np.sum(np.square(weights[i]))\n",
        "    loss_val=loss_val+(L2_lamb/(2*data_size))*square_sum\n",
        "    return loss_val\n",
        "    \n",
        "\n",
        "def calc_accuracy(y,predicted_y):\n",
        "    correct=0\n",
        "    for i in range(len(y)):\n",
        "        if(y[i]==predicted_y[i]):\n",
        "            correct+=1\n",
        "    return (correct/len(y))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI4StwT2MD85"
      },
      "source": [
        "## FUNCTIONS FOR UPDATION OF PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdH8lFBB698B"
      },
      "outputs": [],
      "source": [
        "def sgd_params_update(weights,biases,gradients_W,gradients_B,eta,L,L2_lamb):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      weights[i]=weights[i]-eta*gradients_W[i] -(eta * L2_lamb * weights[i])\n",
        "      biases[i]=biases[i]-eta*gradients_B[i]\n",
        "    return weights,biases\n",
        "\n",
        "\n",
        "def update_parameters_momentum(weights, biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta,L,L2_lamb):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      previous_updates_W[i]=beta*previous_updates_W[i]+(1-beta)*gradients_W[i]\n",
        "      previous_updates_B[i]=beta*previous_updates_B[i]+(1-beta)*gradients_B[i]\n",
        "      weights[i]=weights[i]-eta*previous_updates_W[i]-(eta * L2_lamb * weights[i])\n",
        "      biases[i]=biases[i]-eta*previous_updates_B[i]\n",
        "    return weights,biases,previous_updates_W,previous_updates_B\n",
        "\n",
        "\n",
        "def update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B,t,L,L2_lamb,beta1,beta2,epsilon):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      m_dw=beta1*m_W[i]+(1-beta1)*gradients_W[i]\n",
        "      v_dw=beta2*v_W[i]+(1-beta2)*np.square(gradients_W[i])\n",
        "      m_W_hat=m_dw/(1.0 -beta1**t)\n",
        "      v_W_hat=v_dw/(1.0 -beta2**t)\n",
        "      v_W[i]=v_dw\n",
        "      m_W[i]=m_dw\n",
        "      m_db=beta1*m_B[i]+(1-beta1)*gradients_B[i]\n",
        "      v_db=beta2*v_B[i]+(1-beta2)*np.square(gradients_B[i])\n",
        "      m_B_hat=m_db/(1.0 -beta1**t)\n",
        "      v_B_hat=v_db/(1.0 -beta2**t)\n",
        "      weights[i]=weights[i]-(eta*m_W_hat)/np.sqrt(v_W_hat+epsilon)-(eta * L2_lamb * weights[i])\n",
        "      biases[i]=biases[i]-(eta*m_B_hat)/np.sqrt(v_B_hat+epsilon)\n",
        "      v_B[i]=v_db\n",
        "      m_B[i]=m_db\n",
        "    t=t+1\n",
        "    return weights,biases,m_W,m_B,v_W,v_B,t\n",
        "\n",
        "\n",
        "def rmsprop_params_update(weights, biases, gradients_B,gradients_W, beta,eta, W_v,B_v,L,L2_lamb):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      v_dw= beta*W_v[i]+np.multiply(gradients_W[i],gradients_W[i])\n",
        "      v_db= beta*B_v[i]+np.multiply(gradients_B[i],gradients_B[i])\n",
        "      weights[i]=weights[i]-(eta*(gradients_W[i]))/np.sqrt((v_dw)+1e-4)-(eta * L2_lamb * weights[i])\n",
        "      biases[i]=biases[i]-(eta*(gradients_B[i]))/np.sqrt((v_db)+1e-4)\n",
        "      W_v[i]=v_dw\n",
        "      B_v[i]=v_db\n",
        "    return weights,biases,W_v,B_v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hmUF-x698F"
      },
      "source": [
        "### LEARNING PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4cOoKv698G"
      },
      "outputs": [],
      "source": [
        "def learning_params(hidden_layers,neuron,x_train,y_train,x_val,y_val,learning_algorithm,eta,epochs,batch_size,activation,init_method,L2_lamb,momemtum=0.9 ,beta=0.9 ,beta1=0.9 ,beta2=0.99 ,epsilon=0.00001):\n",
        "  count=1\n",
        "  predicted_y=[]\n",
        "  L=hidden_layers+1\n",
        "  neurons=[0]*(L)\n",
        "  for i in range(1,L):\n",
        "    neurons[i]=neuron\n",
        "  exp_y=one_hot_encoding(y_train)\n",
        "  exp_y_val=one_hot_encoding(y_val)\n",
        "  weights,biases,previous_updates_W,previous_updates_B=initialize_params(hidden_layers,neurons,init_method)\n",
        "  epoch_train_loss=[]\n",
        "  epoch_val_loss=[]\n",
        "  acc_val=[]\n",
        "  acc_train=[]\n",
        "  t=1\n",
        "  v_W = previous_updates_W.copy()\n",
        "  m_W = previous_updates_W.copy()\n",
        "  v_B = previous_updates_B.copy()\n",
        "  m_B = previous_updates_B.copy()\n",
        "  while count<=epochs:\n",
        "      for i in range(0,x_train.shape[1],batch_size):\n",
        "        mini_batch=x_train[:,i:i+batch_size]\n",
        "        if learning_algorithm=='nag':\n",
        "          W_look_ahead=weights-(beta)*previous_updates_W\n",
        "          B_look_ahead=biases-(beta)*previous_updates_B\n",
        "          output,post_act,pre_act=FeedForwardNetwork(W_look_ahead,B_look_ahead,L,mini_batch,activation)\n",
        "          gradients_W,gradients_B=BackPropogation(W_look_ahead,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "          weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights,biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta,L,L2_lamb)\n",
        "        elif learning_algorithm=='nadam':\n",
        "          W_look_ahead=weights-(beta)*previous_updates_W\n",
        "          B_look_ahead=biases-(beta)*previous_updates_B\n",
        "          output,post_act,pre_act=FeedForwardNetwork(W_look_ahead,B_look_ahead,L,mini_batch,activation)\n",
        "          gradients_W,gradients_B=BackPropogation(W_look_ahead,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "          weights,biases,m_W,m_B,v_W,v_B,t= update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B, t,L,L2_lamb,beta1,beta2,epsilon)\n",
        "        elif learning_algorithm=='momemtum':\n",
        "            output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch,activation)\n",
        "            gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "            weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights, biases, gradients_B,gradients_W, momemtum, previous_updates_W,previous_updates_B,eta,L,L2_lamb)\n",
        "        elif learning_algorithm=='sgd':\n",
        "            output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch,activation)\n",
        "            gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "            weights,biases=sgd_params_update(weights,biases,gradients_W,gradients_B,eta,L,L2_lamb)\n",
        "        elif learning_algorithm=='adam':\n",
        "            output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch,activation)\n",
        "            gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "            weights,biases,m_W,m_B,v_W,v_B,t= update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B, t,L,L2_lamb,beta1,beta2,epsilon)\n",
        "        elif learning_algorithm=='rmsprop':\n",
        "            output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch,activation)\n",
        "            gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output,activation)\n",
        "            weights,biases,previous_updates_W,previous_updates_B = rmsprop_params_update(weights, biases, gradients_B,gradients_W, beta,eta, previous_updates_W,previous_updates_B,L,L2_lamb)\n",
        "        else:\n",
        "            break;\n",
        "      full_output_train,_,_=FeedForwardNetwork(weights,biases,L,x_train,activation)\n",
        "      full_output_val,_,_=FeedForwardNetwork(weights,biases,L,x_val,activation)\n",
        "      loss_train=calc_loss(weights,full_output_train,exp_y,'cross_entropy',full_output_train.shape[1],L2_lamb)\n",
        "      loss_val=calc_loss(weights,full_output_val,exp_y_val,'cross_entropy',full_output_val.shape[1],L2_lamb)\n",
        "      acc_train.append(calc_accuracy(y_train,np.argmax(full_output_train,axis=0)))\n",
        "      acc_val.append(calc_accuracy(y_val,np.argmax(full_output_val,axis=0)))\n",
        "      epoch_train_loss.append(loss_train)\n",
        "      epoch_val_loss.append(loss_val)\n",
        "      count+=1\n",
        "  return weights,biases,epoch_train_loss,epoch_val_loss,acc_train,acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_sweeps(train_x,train_y,val_x,val_y):\n",
        "\n",
        "    config = {\n",
        "        \"project\":\"CS6910_Assignment-1\",\n",
        "        \"method\": 'random',\n",
        "        \"metric\": {\n",
        "        'name': 'acc',\n",
        "        'goal': 'maximize'\n",
        "        },\n",
        "        'parameters' :{\n",
        "        \"hidden_layers\": {\"values\":[3,4,5,6]},\n",
        "        \"neurons\": {\"values\": [32,64,128]},\n",
        "        \"learning_algorithm\": {\"values\":[\"momemtum\",\"sgd\",\"nag\",\"rmsprop\",\"nadam\",\"adam\"]},\n",
        "        \"eta\": {\"values\":[1e-3,1e-4]},\n",
        "        \"epoch\": {\"values\":[5,10]},\n",
        "        \"batch_size\": {\"values\":[16,32,64]},\n",
        "        \"activation\": {\"values\":[\"tanh\",\"relu\",\"sigmoid\"]},\n",
        "        \"weight_init\":{\"values\":[\"random\",\"xavier\"]},\n",
        "        \"L2_lamb\":{\"values\":[0,0.0005,0.5]}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def trainn():\n",
        "        wandb.init()\n",
        "        name='_h1_'+str(wandb.config.hidden_layers)+\"_SL_\"+str(wandb.config.neurons)+\"_BS_\"+str(wandb.config.batch_size)+\"_OPT_\"+str(wandb.config.learning_algorithm)\n",
        "        _,_,epoch_train_loss,epoch_val_loss,acc_train,acc_val=learning_params(wandb.config.hidden_layers,wandb.config.neurons,train_x,train_y,val_x,val_y,wandb.config.learning_algorithm,wandb.config.eta,wandb.config.epoch,wandb.config.batch_size,wandb.config.activation,wandb.config.weight_init,wandb.config.L2_lamb)\n",
        "        for i in range(len(epoch_train_loss)):\n",
        "            wandb.log({\"loss\":epoch_train_loss[i],\"epoch\":(i+1)})\n",
        "        for i in range(len(epoch_val_loss)):\n",
        "            wandb.log({\"val_loss\":epoch_val_loss[i],\"epoch\":(i+1)})\n",
        "        for i in range(len(acc_train)):\n",
        "            wandb.log({\"accuracy\":acc_train[i],\"epoch\":(i+1)})\n",
        "        for i in range(len(acc_val)):\n",
        "            wandb.log({\"val_acc\":acc_val[i],\"epoch\":(i+1)})\n",
        "        wandb.log({\"acc\":acc_val[-1]})\n",
        "        wandb.run.name = name\n",
        "        wandb.run.save()\n",
        "        wandb.run.finish()\n",
        "    sweep_id=wandb.sweep(config,project=\"CS6910_Assignment-1\")\n",
        "    wandb.agent(sweep_id,function=trainn,count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    plot_images()\n",
        "    _,_,train_x,train_y,val_x,val_y=data_preprocess()\n",
        "    run_sweeps(train_x,train_y,val_x,val_y)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "_,_,train_x,train_y,val_x,val_y=data_preprocess()\n",
        "weights,biases,epoch_train_loss,epoch_val_loss,acc_train,acc_val=learning_params(hidden_layers=3,neuron=32,x_train=train_x,y_train=train_y,x_val=val_x,y_val=val_y,learning_algorithm=\"sgd\",eta=0.001,epochs=10,batch_size=32,activation=\"sigmoid\",init_method=\"xavier\",L2_lamb=0.01,momemtum=0.9 ,beta=0.9 ,beta1=0.9 ,beta2=0.99 ,epsilon=0.00001)\n",
        "full_output_train,_,_=FeedForwardNetwork(weights,biases,4,train_x,\"sigmoid\")\n",
        "predicted_y=np.argmax(full_output_train,axis=0)\n",
        "predicted_y=np.array(predicted_y,dtype=object)\n",
        "pred_y=predicted_y\n",
        "p_y=pred_y.tolist()\n",
        "y_t=train_y.tolist()\n",
        "conf= metrics.confusion_matrix(p_y,y_t)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf)\n",
        "cm_display.plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f029a96fcde925865f209d04c046aca0c9550919b0e4a2b27b2863cc9a9c3f3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
