{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMEeBJLTeyKB"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SfQKQYGw697b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "t_EBHCoH697m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKAtDnkJxwF"
      },
      "source": [
        "### DOWNLOADING THE DATASET AND PLOTTING ONE SAMPLE IMAGE FROM EACH CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFpHsIZX697r"
      },
      "outputs": [],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"CS6910_Assignment-1\",\n",
        ")\n",
        "\n",
        "# simulate training\n",
        "# x_train is a (60000,28,28) matrix consisting image pixels for training\n",
        "# y_train is a (60000,1) matrix consisting of labels\n",
        "# x_test is a (10000,28,28) matrix consisting image pixels for testing\n",
        "# y_test is a (10000,1) matrix consisting of labels for testing\n",
        "(x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()\n",
        "#NORMALIZING THE DATASET\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "labels=set()\n",
        "i=0\n",
        "fig,ax=plt.subplots(2,5,figsize=(10,5))\n",
        "row=0\n",
        "col=0\n",
        "for pixels in x_train:\n",
        "  #The matplotlib function imshow() creates an image from a 2-dimensional numpy array\n",
        "  #pixels is (28,28) 2-D array\n",
        "  #l is the current label of image\n",
        "  l=y_train[i]\n",
        "  if(not(l in labels)):\n",
        "    if(col>=5):\n",
        "      col=0\n",
        "      row+=1\n",
        "    ax[row][col].imshow(pixels,cmap=\"gray\")\n",
        "    ax[row][col].set_title(\"Label {}\".format(l))\n",
        "    ax[row][col].axis(False)\n",
        "    labels.add(l)\n",
        "    col+=1\n",
        "  #if we get all our 10 labels just break the loop\n",
        "  if(len(labels)==10):\n",
        "    break;\n",
        "  i+=1\n",
        "wandb.log({\"plot\":plt})   \n",
        "# finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99oM1GulHrTg"
      },
      "source": [
        "## DATA-PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "tkEiIJbr697v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 60000)\n"
          ]
        }
      ],
      "source": [
        "#RESHAPING THE TRAIN_IMAGE DATASET FROM (60000,28x28) TO (60000,784) AND SAME FOR TEST_IMAGE\n",
        "num_inputs=784\n",
        "num_outputs=10\n",
        "x_train=x_train.reshape(60000,784)\n",
        "x_test=x_test.reshape(10000,784)\n",
        "\n",
        "\n",
        "#SPLITTING THE TRAINING DATA FOR VALIDATION AND TESTING\n",
        "train_x,val_x,train_y,val_y=train_test_split(x_train,y_train)\n",
        "train_x=np.transpose(train_x)\n",
        "train_y=np.transpose(train_y)\n",
        "val_x=np.transpose(val_x)\n",
        "val_y=np.transpose(val_y)\n",
        "\n",
        "#ONE-HOT ENCODING FOR Y_TRAIN AND Y_TEST: \n",
        "exp_y=np.zeros((10,60000))\n",
        "exp_y_test=np.zeros((10,10000))\n",
        "for i in range(0,60000):\n",
        "    exp_y[y_train[i]][i]=1\n",
        "for i in range(0,10000):\n",
        "    exp_y_test[y_test[i]][i]=1\n",
        "\n",
        "#RESHAPING MY DATA TO COLUMN-WISE IMAGES\n",
        "x_train=x_train.T\n",
        "x_test=x_test.T\n",
        "print(x_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsTgloNKlu1"
      },
      "source": [
        "### ACTIVATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "0rjPkhE9697x"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return np.multiply(s, np.subtract(1, s))\n",
        "\n",
        "def softmax(x):\n",
        "    x=x-np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwJriuPKz-F"
      },
      "source": [
        "### INITIALIZING THE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "AWQEwczB6972"
      },
      "outputs": [],
      "source": [
        "def initialize_params(hidden_layers,neurons):\n",
        "  #USING XAVIER INITIALIZATION TO INITIALIZE WEIGHTS AND BIAS MATRIX\n",
        "\n",
        "  #INDEXING DONE FROM 1\n",
        "  L=hidden_layers+1 #number of layers excluding hidden layer\n",
        "  weights=[0]*(hidden_layers+2)\n",
        "  biases=[0]*(hidden_layers+2)\n",
        "  previous_updates_W=[0]*(hidden_layers+2)\n",
        "  previous_updates_B=[0]*(hidden_layers+2)\n",
        "  np.random.seed(42)\n",
        "  for i in range(1,hidden_layers+1):\n",
        "    n=neurons[i]\n",
        "    # appending the weight and bias matrix for the ith layer\n",
        "    if(i==1):\n",
        "      weights[i]=(np.random.randn(n,784)*np.sqrt(2/(n+784)))\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,784))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "    else:\n",
        "      weights[i]=(np.random.randn(n,neurons[i-1])*np.sqrt(2/(n+neurons[i-1])))\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,neurons[i-1]))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "  weights[L]=(np.random.randn(10,neurons[hidden_layers])*np.sqrt(2/(10+neurons[hidden_layers-1])))\n",
        "  biases[L]=(np.zeros((10,1)))\n",
        "  previous_updates_W[L]=np.zeros((10,neurons[hidden_layers]))\n",
        "  previous_updates_B[L]=np.zeros((10,1))\n",
        "  weights=np.array(weights,dtype=object)\n",
        "  biases=np.array(biases,dtype=object)\n",
        "  previous_updates_W=np.array(previous_updates_W,dtype=object)\n",
        "  previous_updates_B=np.array(previous_updates_B,dtype=object)\n",
        "  return weights,biases,previous_updates_W,previous_updates_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAe2kdSTLThK"
      },
      "source": [
        "## FEED FORWARD PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "wYfoVYJm6973"
      },
      "outputs": [],
      "source": [
        "def FeedForwardNetwork(weights,biases,L,data):\n",
        "  #Returns the array containing the output probablity for each class the data can belong\n",
        "  a=[None]*(L+1)\n",
        "  h=[None]*(L+1)\n",
        "  h[0]=data\n",
        "  for i in range(1,L):\n",
        "    #weight and bias matrix for hidden-layer i\n",
        "    weight=weights[i]\n",
        "    bias=biases[i]\n",
        "    #pre activation\n",
        "    a[i]=(bias+np.matmul(weight,h[i-1]))\n",
        "    #post activation\n",
        "    h[i]=(sigmoid(a[i]))\n",
        "  #weight and bias matrix for ouput layer\n",
        "  weight=weights[L]\n",
        "  bias=biases[L]\n",
        "  a[L]=(bias+np.matmul(weight,h[L-1]))\n",
        "  h[L]=softmax(a[L])\n",
        "  return h[L],h,a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MhyJnsLm2w"
      },
      "source": [
        "## BACK PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "4KvkdzaQ6975"
      },
      "outputs": [],
      "source": [
        "def BackPropogation(weights,L,H,A,exp_Y,y_hat):\n",
        "  # Input Parameters of function\n",
        "  # --> Model Parameters: weights and biases\n",
        "  # --> H: Post-activations, A: Pre-Activations\n",
        "  # --> exp_Y: Actual Class Labels\n",
        "  # Returns list of gradients for each layer wrt weights and biases(parameters)\n",
        "  gradients_H=[0] * (L+1)\n",
        "  gradients_W=[0] * (L+1)\n",
        "  gradients_B=[0] * (L+1)\n",
        "  gradients_A=[0] * (L+1)\n",
        "  \n",
        "  #Computing Gradient For The Output Layer(Pre Activation)\n",
        "  gradients_A[L]=-(exp_Y-y_hat)\n",
        "  for k in range(L,0,-1):\n",
        "    #compute gradients of the parameters\n",
        "    gradients_W[k]=np.matmul(gradients_A[k],np.transpose(H[k-1]))\n",
        "    gradients_B[k]=np.sum(gradients_A[k], axis=1,keepdims=True)\n",
        "    #compute gradients of the hidden layers\n",
        "    gradients_H[k-1]=np.matmul(np.transpose(weights[k]),gradients_A[k])\n",
        "    if(k>1):\n",
        "      gradients_A[k-1]=np.multiply(gradients_H[k-1],sigmoid_derivative(A[k-1]))\n",
        "    \n",
        "\n",
        "  return gradients_W,gradients_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI4StwT2MD85"
      },
      "source": [
        "## FUNCTIONS FOR UPDATION OF PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "YdH8lFBB698B"
      },
      "outputs": [],
      "source": [
        "def sgd_params_update(weights,biases,gradients_W,gradients_B,eta):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    weights=weights-eta*(gradients_W)\n",
        "    biases=biases-eta*(gradients_B)\n",
        "    return weights,biases\n",
        "\n",
        "\n",
        "\n",
        "def update_parameters_momentum(weights, biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    previous_updates_W=beta*previous_updates_W+gradients_W\n",
        "    previous_updates_B=beta*previous_updates_B+gradients_B\n",
        "    weights=weights-eta*previous_updates_W\n",
        "    biases=biases-eta*previous_updates_B\n",
        "    return weights,biases,previous_updates_W,previous_updates_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hmUF-x698F"
      },
      "source": [
        "### LEARNING PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "bx4cOoKv698G"
      },
      "outputs": [],
      "source": [
        "def learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size):\n",
        "  count=1\n",
        "  predicted_y=[]\n",
        "  weights,biases,previous_updates_W,previous_updates_B=initialize_params(hidden_layers,neurons)\n",
        "  L=hidden_layers+1\n",
        "  while count<=epochs:\n",
        "      for i in range(0,x_train.shape[1],batch_size):\n",
        "        mini_batch=x_train[:,i:i+batch_size]\n",
        "        if learning_algorithm=='nestrov_accelerated_gradient_descent':\n",
        "          beta=0.001\n",
        "          W_look_ahead=weights-(beta)*previous_updates_W\n",
        "          B_look_ahead=biases-(beta)*previous_updates_B\n",
        "          # print(W_look_ahead[1])\n",
        "          output,post_act,pre_act=FeedForwardNetwork(W_look_ahead,B_look_ahead,L,mini_batch)\n",
        "          gradients_W,gradients_B=BackPropogation(W_look_ahead,L,post_act,pre_act,exp_y[:,i:i+batch_size],output)\n",
        "          weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights,biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta)\n",
        "        else:\n",
        "          output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch)\n",
        "          gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output)\n",
        "          if learning_algorithm=='momemtum_gradient_descent':\n",
        "            beta=0.001\n",
        "            weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights, biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta)\n",
        "          elif learning_algorithm=='stochastic_gradient_descent':\n",
        "            weights,biases=sgd_params_update(weights,biases,gradients_W,gradients_B,eta)\n",
        "          \n",
        "      # print(weights[1])\n",
        "      full_output,_,_=FeedForwardNetwork(weights,biases,L,x_train)\n",
        "      predicted_y.append(np.argmax(full_output,axis=0))\n",
        "      count+=1\n",
        "  # print(full_output)\n",
        "  return weights,biases,predicted_y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AHPhRMxf698B"
      },
      "source": [
        "## **MODEL 1**\n",
        "---\n",
        "MOMEMTUM GRADIENT DESCENT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "I0WHn8gy698E"
      },
      "outputs": [],
      "source": [
        "batch_size=512\n",
        "epochs=10\n",
        "eta=0.001\n",
        "learning_algorithm=\"momemtum_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=[0,128,128,128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "6vrjjGQBIkEC"
      },
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pxxJyKe8SkCC"
      },
      "source": [
        "## **MODEL 2**\n",
        "---\n",
        "STOCHASTIC GRADIENT DESCENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "mAglfVSbSi9s"
      },
      "outputs": [],
      "source": [
        "batch_size=512\n",
        "epochs=42\n",
        "eta=0.001\n",
        "learning_algorithm=\"stochastic_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=[0,128,128,128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3\n",
        "\n",
        "----\n",
        "Nestrov Accelerated Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=512\n",
        "epochs=42\n",
        "eta=0.001\n",
        "learning_algorithm=\"nestrov_accelerated_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=[0,128,128,128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxsg-bE-DoXD",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "# print(weights)\n",
        "for i in range(0,len(predicted_y)):\n",
        "    print(\"Training accuracy = {} %\".format(round(accuracy_score(y_train, predicted_y[i]) * 100, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w576e1Qk698A",
        "outputId": "dc607375-5a24-462c-bf34-f82923173430"
      },
      "outputs": [],
      "source": [
        "predicted_y_temp=[]\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "output,post_act,pre_act=FeedForwardNetwork(weights,biases,hidden_layers+1,x_test)\n",
        "print(output.shape)\n",
        "predicted_y_temp.append(np.argmax(output,axis=0))\n",
        "for i in range(0,len(predicted_y_temp)):\n",
        "    print(predicted_y_temp[i])\n",
        "    print(\"Testing accuracy = {} %\".format(round(accuracy_score(y_test, predicted_y_temp[i]) * 100, 3)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f029a96fcde925865f209d04c046aca0c9550919b0e4a2b27b2863cc9a9c3f3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
