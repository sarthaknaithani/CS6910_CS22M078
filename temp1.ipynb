{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vMEeBJLTeyKB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.10)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (63.2.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.9.1)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.5)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\sarthak naithani\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SfQKQYGw697b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t_EBHCoH697m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKAtDnkJxwF"
      },
      "source": [
        "### DOWNLOADING THE DATASET AND PLOTTING ONE SAMPLE IMAGE FROM EACH CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFpHsIZX697r"
      },
      "outputs": [],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"CS6910_Assignment-1\",\n",
        ")\n",
        "\n",
        "# simulate training\n",
        "# x_train is a (60000,28,28) matrix consisting image pixels for training\n",
        "# y_train is a (60000,1) matrix consisting of labels\n",
        "# x_test is a (10000,28,28) matrix consisting image pixels for testing\n",
        "# y_test is a (10000,1) matrix consisting of labels for testing\n",
        "(x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()\n",
        "labels=set()\n",
        "i=0\n",
        "fig,ax=plt.subplots(2,5,figsize=(10,5))\n",
        "row=0\n",
        "col=0\n",
        "for pixels in x_train:\n",
        "  #The matplotlib function imshow() creates an image from a 2-dimensional numpy array\n",
        "  #pixels is (28,28) 2-D array\n",
        "  #l is the current label of image\n",
        "  l=y_train[i]\n",
        "  if(not(l in labels)):\n",
        "    if(col>=5):\n",
        "      col=0\n",
        "      row+=1\n",
        "    ax[row][col].imshow(pixels,cmap=\"gray\")\n",
        "    ax[row][col].set_title(\"Label {}\".format(l))\n",
        "    ax[row][col].axis(False)\n",
        "    labels.add(l)\n",
        "    col+=1\n",
        "  #if we get all our 10 labels just break the loop\n",
        "  if(len(labels)==10):\n",
        "    break;\n",
        "  i+=1\n",
        "wandb.log({\"plot\":plt})   \n",
        "# finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99oM1GulHrTg"
      },
      "source": [
        "## DATA-PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkEiIJbr697v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()\n",
        "#NORMALIZING THE DATASET\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "#RESHAPING THE TRAIN_IMAGE DATASET FROM (60000,28x28) TO (60000,784) AND SAME FOR TEST_IMAGE\n",
        "num_inputs=784\n",
        "num_outputs=10\n",
        "x_train=x_train.reshape(60000,784)\n",
        "x_test=x_test.reshape(10000,784)\n",
        "\n",
        "\n",
        "#SPLITTING THE TRAINING DATA FOR VALIDATION AND TESTING\n",
        "train_x,val_x,train_y,val_y=train_test_split(x_train,y_train)\n",
        "train_x=np.transpose(train_x)\n",
        "train_y=np.transpose(train_y)\n",
        "val_x=np.transpose(val_x)\n",
        "val_y=np.transpose(val_y)\n",
        "\n",
        "#ONE-HOT ENCODING FOR Y_TRAIN AND Y_TEST: \n",
        "def one_hot_encoding(y):\n",
        "    print(y.shape[0])\n",
        "    exp_y=np.zeros((10,y.shape[0]))\n",
        "    for i in range(0,y.shape[0]):\n",
        "        exp_y[y[i]][i]=1\n",
        "    return exp_y\n",
        "#RESHAPING MY DATA TO COLUMN-WISE IMAGES\n",
        "x_train=x_train.T\n",
        "x_test=x_test.T\n",
        "print(y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsTgloNKlu1"
      },
      "source": [
        "### ACTIVATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rjPkhE9697x"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return np.multiply(s, np.subtract(1, s))\n",
        "\n",
        "def softmax(x):\n",
        "    x=x-np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def Relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def Relu_derivative(x):\n",
        "    return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwJriuPKz-F"
      },
      "source": [
        "### INITIALIZING THE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWQEwczB6972"
      },
      "outputs": [],
      "source": [
        "def initialize_params(hidden_layers,neurons):\n",
        "  #USING XAVIER INITIALIZATION TO INITIALIZE WEIGHTS AND BIAS MATRIX\n",
        "\n",
        "  #INDEXING DONE FROM 1\n",
        "  L=hidden_layers+1 #number of layers excluding hidden layer\n",
        "  weights=[0]*(hidden_layers+2)\n",
        "  biases=[0]*(hidden_layers+2)\n",
        "  previous_updates_W=[0]*(hidden_layers+2)\n",
        "  previous_updates_B=[0]*(hidden_layers+2)\n",
        "  np.random.seed(42)\n",
        "  for i in range(1,hidden_layers+1):\n",
        "    n=neurons[i]\n",
        "    # appending the weight and bias matrix for the ith layer\n",
        "    if(i==1):\n",
        "      weights[i]=(np.random.randn(n,784)*np.sqrt(2/(n+784)))\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,784))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "    else:\n",
        "      weights[i]=(np.random.randn(n,neurons[i-1])*np.sqrt(2/(n+neurons[i-1])))\n",
        "      biases[i]=(np.zeros((n,1)))\n",
        "      previous_updates_W[i]=np.zeros((n,neurons[i-1]))\n",
        "      previous_updates_B[i]=np.zeros((n,1))\n",
        "      # biases[i]=(np.random.randn(n,1))\n",
        "  weights[L]=(np.random.randn(10,neurons[hidden_layers])*np.sqrt(2/(10+neurons[hidden_layers-1])))\n",
        "  biases[L]=(np.zeros((10,1)))\n",
        "  previous_updates_W[L]=np.zeros((10,neurons[hidden_layers]))\n",
        "  previous_updates_B[L]=np.zeros((10,1))\n",
        "  weights=np.array(weights,dtype=object)\n",
        "  biases=np.array(biases,dtype=object)\n",
        "  previous_updates_W=np.array(previous_updates_W,dtype=object)\n",
        "  previous_updates_B=np.array(previous_updates_B,dtype=object)\n",
        "  return weights,biases,previous_updates_W,previous_updates_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAe2kdSTLThK"
      },
      "source": [
        "## FEED FORWARD PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYfoVYJm6973"
      },
      "outputs": [],
      "source": [
        "def FeedForwardNetwork(weights,biases,L,data):\n",
        "  #Returns the array containing the output probablity for each class the data can belong\n",
        "  a=[0]*(L+1)\n",
        "  h=[0]*(L+1)\n",
        "  h[0]=data\n",
        "  for i in range(1,L):\n",
        "    #weight and bias matrix for hidden-layer i\n",
        "    weight=weights[i]\n",
        "    bias=biases[i]\n",
        "    #pre activation\n",
        "    a[i]=(bias+np.matmul(weight,h[i-1]))\n",
        "    #post activation\n",
        "    h[i]=(sigmoid(a[i]))\n",
        "  #weight and bias matrix for ouput layer\n",
        "  weight=weights[L]\n",
        "  bias=biases[L]\n",
        "  a[L]=(bias+np.matmul(weight,h[L-1]))\n",
        "  h[L]=softmax(a[L])\n",
        "  return h[L],h,a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MhyJnsLm2w"
      },
      "source": [
        "## BACK PROPOGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KvkdzaQ6975"
      },
      "outputs": [],
      "source": [
        "def BackPropogation(weights,L,H,A,exp_Y,y_hat):\n",
        "  # Input Parameters of function\n",
        "  # --> Model Parameters: weights and biases\n",
        "  # --> H: Post-activations, A: Pre-Activations\n",
        "  # --> exp_Y: Actual Class Labels\n",
        "  # Returns list of gradients for each layer wrt weights and biases(parameters)\n",
        "  gradients_H=[0] * (L+1)\n",
        "  gradients_W=[0] * (L+1)\n",
        "  gradients_B=[0] * (L+1)\n",
        "  gradients_A=[0] * (L+1)\n",
        "  \n",
        "  #Computing Gradient For The Output Layer(Pre Activation)\n",
        "  gradients_A[L]=-(exp_Y-y_hat)\n",
        "  for k in range(L,0,-1):\n",
        "    #compute gradients of the parameters\n",
        "    gradients_W[k]=np.matmul(gradients_A[k],np.transpose(H[k-1]))\n",
        "    gradients_B[k]=np.sum(gradients_A[k], axis=1,keepdims=True)\n",
        "    #compute gradients of the hidden layers\n",
        "    gradients_H[k-1]=np.matmul(np.transpose(weights[k]),gradients_A[k])\n",
        "    if(k>1):\n",
        "      gradients_A[k-1]=np.multiply(gradients_H[k-1],sigmoid_derivative(A[k-1]))\n",
        "    \n",
        "\n",
        "  return gradients_W,gradients_B"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss(y,exp_y,loss,batch_size):\n",
        "    #Calculating -log(p(x)) where p(x) is probablity where our of our actual class label of x\n",
        "    # exp_y contains y=1 if y is actual class of x else 0\n",
        "    # so dot product gives only the log of the probablity of actual class of x\n",
        "    if loss=='cross_entropy':\n",
        "        p_x= np.multiply(exp_y,np.log(y))\n",
        "        return -np.sum(p_x)/60000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI4StwT2MD85"
      },
      "source": [
        "## FUNCTIONS FOR UPDATION OF PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdH8lFBB698B"
      },
      "outputs": [],
      "source": [
        "def sgd_params_update(weights,biases,gradients_W,gradients_B,eta):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    weights=weights-eta*(gradients_W)\n",
        "    biases=biases-eta*(gradients_B)\n",
        "    return weights,biases\n",
        "\n",
        "\n",
        "\n",
        "def update_parameters_momentum(weights, biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta,L):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      previous_updates_W[i]=beta*previous_updates_W[i]+(1-beta)*gradients_W[i]\n",
        "      previous_updates_B[i]=beta*previous_updates_B[i]+(1-beta)*gradients_B[i]\n",
        "      weights[i]=weights[i]-eta*previous_updates_W[i]\n",
        "      biases[i]=biases[i]-eta*previous_updates_B[i]\n",
        "    return weights,biases,previous_updates_W,previous_updates_B\n",
        "\n",
        "\n",
        "def update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B,t,L):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.99\n",
        "    epsilon = 1e-4 \n",
        "    for i in range(1,L+1):\n",
        "      m_dw=beta1*m_W[i]+(1-beta1)*gradients_W[i]\n",
        "      v_dw=beta2*v_W[i]+(1-beta2)*np.square(gradients_W[i])\n",
        "      m_W_hat=m_dw/(1.0 -beta1**t)\n",
        "      v_W_hat=v_dw/(1.0 -beta2**t)\n",
        "      v_W[i]=v_dw\n",
        "      m_W[i]=m_dw\n",
        "      m_db=beta1*m_B[i]+(1-beta1)*gradients_B[i]\n",
        "      v_db=beta2*v_B[i]+(1-beta2)*np.square(gradients_B[i])\n",
        "      m_B_hat=m_db/(1.0 -beta1**t)\n",
        "      v_B_hat=v_db/(1.0 -beta2**t)\n",
        "      weights[i]=weights[i]-(eta*m_W_hat)/np.sqrt(v_W_hat+epsilon)\n",
        "      biases[i]=biases[i]-(eta*m_B_hat)/np.sqrt(v_B_hat+epsilon)\n",
        "      v_B[i]=v_db\n",
        "      m_B[i]=m_db\n",
        "    t=t+1\n",
        "    return weights,biases,m_W,m_B,v_W,v_B,t\n",
        "\n",
        "\n",
        "def rmsprop_params_update(weights, biases, gradients_B,gradients_W, beta,eta, W_v,B_v,L):\n",
        "    gradients_B=np.array(gradients_B,dtype=object)\n",
        "    gradients_W=np.array(gradients_W,dtype=object)\n",
        "    for i in range(1,L+1):\n",
        "      v_dw= beta*W_v[i]+np.multiply(gradients_W[i],gradients_W[i])\n",
        "      v_db= beta*B_v[i]+np.multiply(gradients_B[i],gradients_B[i])\n",
        "      weights[i]=weights[i]-(eta*(gradients_W[i]))/np.sqrt((v_dw)+1e-4)\n",
        "      biases[i]=biases[i]-(eta*(gradients_B[i]))/np.sqrt((v_db)+1e-4)\n",
        "      W_v[i]=v_dw\n",
        "      B_v[i]=v_db\n",
        "    return weights,biases,W_v,B_v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hmUF-x698F"
      },
      "source": [
        "### LEARNING PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4cOoKv698G"
      },
      "outputs": [],
      "source": [
        "def learning_params(hidden_layers,neuron,x_train,y_train,x_val,y_val,learning_algorithm,eta,epochs,batch_size):\n",
        "  count=1\n",
        "  predicted_y=[]\n",
        "  L=hidden_layers+1\n",
        "  neurons=[0]*(L)\n",
        "  for i in range(1,L):\n",
        "    neurons[i]=neuron\n",
        "  exp_y=one_hot_encoding(y_train)\n",
        "  exp_y_val=one_hot_encoding(y_val)\n",
        "  weights,biases,previous_updates_W,previous_updates_B=initialize_params(hidden_layers,neurons)\n",
        "  epoch_train_loss=[]\n",
        "  epoch_val_loss=[]\n",
        "  t=1\n",
        "  v_W = previous_updates_W.copy()\n",
        "  m_W = previous_updates_W.copy()\n",
        "  v_B = previous_updates_B.copy()\n",
        "  m_B = previous_updates_B.copy()\n",
        "  beta=0.9\n",
        "  while count<=epochs:\n",
        "      for i in range(0,x_train.shape[1],batch_size):\n",
        "        mini_batch=x_train[:,i:i+batch_size]\n",
        "        if learning_algorithm=='nestrov_accelerated_gradient_descent':\n",
        "          \n",
        "          W_look_ahead=weights-(beta)*previous_updates_W\n",
        "          B_look_ahead=biases-(beta)*previous_updates_B\n",
        "          output,post_act,pre_act=FeedForwardNetwork(W_look_ahead,B_look_ahead,L,mini_batch)\n",
        "          gradients_W,gradients_B=BackPropogation(W_look_ahead,L,post_act,pre_act,exp_y[:,i:i+batch_size],output)\n",
        "          weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights,biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta,L)\n",
        "        elif learning_algorithm=='nadam':\n",
        "          W_look_ahead=weights-(beta)*previous_updates_W\n",
        "          B_look_ahead=biases-(beta)*previous_updates_B\n",
        "          output,post_act,pre_act=FeedForwardNetwork(W_look_ahead,B_look_ahead,L,mini_batch)\n",
        "          gradients_W,gradients_B=BackPropogation(W_look_ahead,L,post_act,pre_act,exp_y[:,i:i+batch_size],output)\n",
        "          weights,biases,m_W,m_B,v_W,v_B,t= update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B, t,L)\n",
        "        else:\n",
        "          output,post_act,pre_act=FeedForwardNetwork(weights,biases,L,mini_batch)\n",
        "          gradients_W,gradients_B=BackPropogation(weights,L,post_act,pre_act,exp_y[:,i:i+batch_size],output)\n",
        "          if learning_algorithm=='momemtum_gradient_descent':\n",
        "            weights,biases,previous_updates_W,previous_updates_B=update_parameters_momentum(weights, biases, gradients_B,gradients_W, beta, previous_updates_W,previous_updates_B,eta,L)\n",
        "          elif learning_algorithm=='stochastic_gradient_descent':\n",
        "            weights,biases=sgd_params_update(weights,biases,gradients_W,gradients_B,eta)\n",
        "          elif learning_algorithm=='adam':\n",
        "            weights,biases,m_W,m_B,v_W,v_B,t= update_parameters_adam(weights, biases, gradients_B,gradients_W,eta, m_W,m_B,v_W,v_B, t,L)\n",
        "          elif learning_algorithm=='rmsprop':\n",
        "            weights,biases,previous_updates_W,previous_updates_B = rmsprop_params_update(weights, biases, gradients_B,gradients_W, beta,eta, previous_updates_W,previous_updates_B,L)          \n",
        "\n",
        "          \n",
        "      # print(weights[1])\n",
        "      full_output_train,_,_=FeedForwardNetwork(weights,biases,L,x_train)\n",
        "      full_output_val,_,_=FeedForwardNetwork(weights,biases,L,x_val)\n",
        "      loss_train=calc_loss(full_output_train,exp_y,'cross_entropy',full_output_train.shape[1])\n",
        "      loss_val=calc_loss(full_output_val,exp_y_val,'cross_entropy',full_output_val.shape[1])\n",
        "      # predicted_y.append(np.argmax(full_output,axis=0))\n",
        "      epoch_train_loss.append(loss_train)\n",
        "      epoch_val_loss.append(loss_val)\n",
        "      print(loss_train)\n",
        "      print(loss_val)\n",
        "      count+=1\n",
        "  return weights,biases,epoch_train_loss,epoch_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHPhRMxf698B"
      },
      "source": [
        "## **MODEL 1**\n",
        "---\n",
        "MOMEMTUM GRADIENT DESCENT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0WHn8gy698E"
      },
      "outputs": [],
      "source": [
        "batch_size=512\n",
        "epochs=10\n",
        "eta=0.001\n",
        "learning_algorithm=\"momemtum_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vrjjGQBIkEC"
      },
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,train_x,train_y,val_x,val_y,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxxJyKe8SkCC"
      },
      "source": [
        "## **MODEL 2**\n",
        "---\n",
        "STOCHASTIC GRADIENT DESCENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAglfVSbSi9s"
      },
      "outputs": [],
      "source": [
        "batch_size=512\n",
        "epochs=42\n",
        "eta=0.001\n",
        "learning_algorithm=\"stochastic_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DJBovrgLQ8E"
      },
      "outputs": [],
      "source": [
        "weights,biases,epoch_train_loss,epoch_val_loss=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IuovO7QLQ8E"
      },
      "source": [
        "## Model 3\n",
        "\n",
        "----\n",
        "Nestrov Accelerated Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bxDftZnLQ8F"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "epochs=10\n",
        "eta=0.01\n",
        "learning_algorithm=\"nestrov_accelerated_gradient_descent\"\n",
        "hidden_layers=3\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIN7gaWELQ8F"
      },
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weCREUaZBMAh"
      },
      "source": [
        "## Model 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "htmlzpNCW3Yt"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "epochs=5\n",
        "eta=0.001\n",
        "learning_algorithm=\"adam\"\n",
        "hidden_layers=4\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OHJ95TV0W3tI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45000\n",
            "15000\n",
            "0.45029344737448057\n",
            "0.15197185073811614\n",
            "0.39111239285977817\n",
            "0.1341424351703946\n",
            "0.3479029124333697\n",
            "0.12144124142775604\n",
            "0.3026443277484638\n",
            "0.10812368764738464\n",
            "0.27689786374139613\n",
            "0.10097102846512174\n"
          ]
        }
      ],
      "source": [
        "weights,biases,epoch_train_loss,epoch_val_loss=learning_params(hidden_layers,neurons,train_x,train_y,val_x,val_y,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9x-_yBMMqM6"
      },
      "source": [
        "## Model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y26pfdobM4Ao"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "epochs=10\n",
        "eta=0.001\n",
        "learning_algorithm=\"nadam\"\n",
        "hidden_layers=3\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxabyrdM8Gz"
      },
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtC5LwwvUr8r"
      },
      "source": [
        "# Model 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rna1LVo-UvHb"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "epochs=10\n",
        "eta=0.001\n",
        "learning_algorithm=\"rmsprop\"\n",
        "hidden_layers=3\n",
        "neurons=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUyKTbD_U6o9"
      },
      "outputs": [],
      "source": [
        "weights,biases,predicted_y=learning_params(hidden_layers,neurons,x_train,y_train,learning_algorithm,eta,epochs,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lw6vC4ULQ8G"
      },
      "source": [
        "## Checking Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxsg-bE-DoXD",
        "outputId": "105ca367-2772-46bc-a48a-b0c641c44ad9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "# print(weights)\n",
        "for i in range(0,len(predicted_y)):\n",
        "    print(\"Training accuracy = {} %\".format(round(accuracy_score(train_y, predicted_y[i]) * 100, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w576e1Qk698A"
      },
      "outputs": [],
      "source": [
        "predicted_y_temp=[]\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "output,post_act,pre_act=FeedForwardNetwork(weights,biases,hidden_layers+1,x_test)\n",
        "print(output.shape)\n",
        "predicted_y_temp.append(np.argmax(output,axis=0))\n",
        "for i in range(0,len(predicted_y_temp)):\n",
        "    print(predicted_y_temp[i])\n",
        "    print(\"Testing accuracy = {} %\".format(round(accuracy_score(y_test, predicted_y_temp[i]) * 100, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32md:\\DL\\CS6910_CS22M078\\temp1.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# def trainn():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#   name='h1'+str(config['hidden_layers'])+\"SL\"+str(config['neurons'])+\"BS\"+str(config['batch_size'])+\"OPT\"+str(config['learning_algorithm'])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#   wandb.init(name=name)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#   learning_params(config['parameters']['hidden_layers'],config['parameters']['neurons'],val_x,val_y,config['parameters']['learning_algorithm'],config['parameters']['eta'],config['parameters']['epoch'],config['parameters']['batch_size'])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m sweep_id\u001b[39m=\u001b[39mwandb\u001b[39m.\u001b[39msweep(config,project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCS6910_Assignment-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DL/CS6910_CS22M078/temp1.ipynb#X61sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m wandb\u001b[39m.\u001b[39magent(sweep_id,function\u001b[39m=\u001b[39mtrainn,count\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ],
      "source": [
        "def trainn():\n",
        "    wandb.init()\n",
        "    name='_h1_'+str(wandb.config.hidden_layers)+\"_SL_\"+str(wandb.config.neurons)+\"_BS_\"+str(wandb.config.batch_size)+\"_OPT_\"+str(wandb.config.learning_algorithm)\n",
        "    wandb.init(name=name)\n",
        "    weights,biases,epoch_train_loss,epoch_val_loss=learning_params(wandb.config.hidden_layers,wandb.config.neurons,train_x,train_y,val_x,val_y,wandb.config.learning_algorithm,wandb.config.eta,wandb.config.epoch,wandb.config.batch_size)\n",
        "    for i in range(len(epoch_train_loss)):\n",
        "        wandb.log({\"loss\":epoch_train_loss[i],\"epoch\":(i+1)})\n",
        "    for i in range(len(epoch_val_loss)):\n",
        "        wandb.log({\"val_loss\":epoch_val_loss[i],\"epoch\":(i+1)})\n",
        "# def trainn():\n",
        "#   name='h1'+str(config['hidden_layers'])+\"SL\"+str(config['neurons'])+\"BS\"+str(config['batch_size'])+\"OPT\"+str(config['learning_algorithm'])\n",
        "#   wandb.init(name=name)\n",
        "#   learning_params(config['parameters']['hidden_layers'],config['parameters']['neurons'],val_x,val_y,config['parameters']['learning_algorithm'],config['parameters']['eta'],config['parameters']['epoch'],config['parameters']['batch_size'])\n",
        "\n",
        "\n",
        "    \n",
        "config = {\n",
        "    \"method\": 'random',\n",
        "    \"metric\": {\n",
        "    'name': 'accuracy',\n",
        "    'goal': 'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "    \"hidden_layers\": {\"values\":[3,4,5,6]},\n",
        "    \"neurons\": {\"values\": [32,64,128]},\n",
        "    \"learning_algorithm\": {\"values\":[\"momemtum_gradient_descent\",\"stochastic_gradient_descent\",\"nestrov_accelerated_gradient_descent\",\"rmsprop\",\"nadam\",\"adam\"]},\n",
        "    \"eta\": {\"values\":[1e-3,1e-4]},\n",
        "    \"epoch\": {\"values\":[5,10]},\n",
        "    \"batch_size\": {\"values\":[16,32,64]}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_id=wandb.sweep(config,project=\"CS6910_Assignment-1\")\n",
        "wandb.agent(sweep_id,function=trainn,count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f029a96fcde925865f209d04c046aca0c9550919b0e4a2b27b2863cc9a9c3f3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
